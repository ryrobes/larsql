# Simple RLM Test Cascade
#
# Tests the RLM pattern: model writes code to decompose context using
# rlm_exec, which provides llm_query() for sub-LLM calls.
#
# Test with:
#   lars run cascades/rlm_simple_test.cascade.yaml \
#     --input cascades/rlm_test_input.json --session rlm_test

cascade_id: rlm_simple_test
description: "Minimal RLM pattern test"

inputs_schema:
  context: "Text to analyze"
  task: "What to do with it"

cells:
  - name: analyze
    model: "google/gemini-2.5-flash-lite"
    instructions: |
      You have a context to analyze. Write Python code to process it using rlm_exec.

      ## Task
      {{ input.task }}

      ## The rlm_exec Tool

      Call rlm_exec with a "code" argument. Inside your code you have:
      - `context`: The input text ({{ input.context | length }} chars)
      - `task`: The task description
      - `llm_query(prompt)`: Make a sub-LLM call, returns string
      - `llm_query_batched([prompts])`: Parallel sub-LLM calls
      - `chunk(text, strategy)`: Split text ("paragraph", "markdown", "sentence", "fixed")
      - `set_state(key, value)`: Store result
      - `results`: List for accumulating findings
      - `print()`: For progress logging

      ## Strategy

      1. Check context size
      2. If small (<2000 chars): analyze directly with llm_query
      3. If large: chunk it, analyze chunks, synthesize

      ## Example Code

      ```python
      # Check size
      print(f"Context: {len(context)} chars")

      if len(context) < 2000:
          # Direct analysis
          answer = llm_query(f"{task}\\n\\nContext: {context}")
          set_state("final_answer", answer)
      else:
          # Chunk and analyze
          chunks = chunk(context, "markdown")
          print(f"Split into {len(chunks)} chunks")

          for i, c in enumerate(chunks[:5]):  # Limit chunks
              summary = llm_query(f"Extract key points: {c[:2000]}")
              results.append(summary)
              print(f"Processed chunk {i+1}")

          # Synthesize
          combined = "\\n".join(results)
          final = llm_query(f"{task}\\n\\nFindings:\\n{combined}")
          set_state("final_answer", final)
      ```

      ## Context Preview
      ```
      {{ input.context[:1000] }}{% if input.context | length > 1000 %}...{% endif %}
      ```

      Now write your code and call rlm_exec. When done, the loop will exit.

    skills: [rlm_exec, set_state]
    rules:
      max_turns: 5
      loop_until: "{{ state.final_answer }}"
