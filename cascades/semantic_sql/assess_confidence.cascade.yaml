# Confidence Assessment - Automatic quality scoring for training examples
# Runs post-execution to assess how confident we are in the result
#
# Used by the training system to auto-populate confidence scores
# for training example curation

cascade_id: assess_training_confidence

description: |
  Assess the confidence/quality of a cascade execution result.
  Returns a score from 0.0 (low confidence) to 1.0 (high confidence).

  Considers:
  - Clarity of the output
  - Consistency with the input
  - Completeness of the response
  - Any obvious errors or hallucinations

inputs_schema:
  user_prompt: The original user prompt/instructions
  assistant_response: The assistant's response/output
  cascade_id: Which cascade generated this (for context)
  cell_name: Which cell generated this (for context)

cells:
  - name: score_confidence
    #model: google/gemini-2.5-flash-lite
    instructions: |
      Assess the quality and confidence of this AI assistant response.

      ORIGINAL PROMPT:
      {{ input.user_prompt }}

      ASSISTANT RESPONSE:
      {{ input.assistant_response }}

      CONTEXT: This was generated by cascade "{{ input.cascade_id }}", cell "{{ input.cell_name }}"

      Rate the confidence/quality from 0.0 (low) to 1.0 (high) based on:
      - Clarity: Is the response clear and well-formed?
      - Correctness: Does it properly address the prompt?
      - Completeness: Is it complete or truncated/partial?
      - Format: Does it follow the expected format (e.g., true/false, JSON array, etc.)?

      Return ONLY a decimal number between 0.0 and 1.0 (e.g., "0.85")
      No explanation, just the score.

    rules:
      max_turns: 1

    output_schema:
      type: number
      minimum: 0.0
      maximum: 1.0
