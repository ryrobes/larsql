cascade_id: embed_batch_elastic
internal: true
description: "Batch embed rows from a table, storing results in Elasticsearch.\n\n\
  Uses Elasticsearch's dense_vector field for hybrid search (vector + keyword).\n\
  This is preferred over ClickHouse when you want BM25 keyword matching combined\n\
  with semantic similarity.\n\nAccepts a JSON array of {id, text} objects. Batches\
  \ API calls (50 texts per call)\nand bulk indexes to Elasticsearch.\n\nSQL Usage:\n\
  \  -- Embed from any table (DuckDB or ClickHouse)\n  SELECT embed_batch_elastic(\n\
  \    'products',\n    'description',\n    (SELECT to_json(list({'id': CAST(id AS\
  \ VARCHAR), 'text': description})) FROM products)\n  );\n\n  -- With custom batch\
  \ size and index name\n  SELECT embed_batch_elastic('products', 'description', (SELECT\
  \ ...), 100, 'my_index');\n\nReturns JSON stats:\n  {\n    \"rows_embedded\": 850,\n\
  \    \"batches\": 17,\n    \"model\": \"qwen/qwen3-embedding-8b\",\n    \"duration_seconds\"\
  : 45.2,\n    \"backend\": \"elasticsearch\"\n  }\n"
inputs_schema:
  table_name: Source table name (for tracking)
  column_name: Column name (for metadata)
  rows_json: JSON array of {id, text} objects
  batch_size: 'Texts per API call (default: 50)'
  index_name: 'Elasticsearch index (default: rvbbit_embeddings)'
sql_function:
  name: embed_batch_elastic
  description: Batch embed rows and store in Elasticsearch for hybrid search
  args:
  - name: table_name
    type: VARCHAR
    description: Source table name (for tracking)
  - name: column_name
    type: VARCHAR
    description: Column name (for metadata)
  - name: rows_json
    type: VARCHAR
    description: JSON array of {id, text} objects
  - name: batch_size
    type: INTEGER
    optional: true
    default: 50
    description: Batch size (default 50)
  - name: index_name
    type: VARCHAR
    optional: true
    default: rvbbit_embeddings
    description: Elasticsearch index name
  returns: VARCHAR
  shape: SCALAR
  cache: false
  test_cases:
  - sql: -- Requires Elasticsearch
    skip: true
    description: Elastic embed needs ES
cells:
- name: batch_embed_elastic
  tool: python_data
  inputs:
    code: "import time\nimport json\nimport logging\nfrom rvbbit.traits.embedding_storage\
      \ import (\n    agent_embed_batch,\n    elasticsearch_bulk_store_embeddings\n\
      )\n\nlogger = logging.getLogger(__name__)\nstart_time = time.time()\n\n# Get\
      \ injected context for SQL Trail correlation\n_session_id = input.get('_session_id')\n\
      _caller_id = input.get('_caller_id')\n_cascade_id = input.get('_cascade_id')\n\
      _cell_name = input.get('_cell_name')\n\ntable_name = input.get('table_name',\
      \ 'unknown')\ncolumn_name = input.get('column_name', 'unknown')\nrows_json =\
      \ input.get('rows_json', '[]')\nbatch_size = input.get('batch_size') or 50\n\
      index_name = input.get('index_name') or 'rvbbit_embeddings'\n\ntry:\n    rows\
      \ = json.loads(rows_json) if isinstance(rows_json, str) else rows_json\nexcept:\n\
      \    result = json.dumps({\"error\": \"Invalid JSON in rows_json\"})\nelse:\n\
      \    if not rows:\n        result = json.dumps({\"rows_embedded\": 0, \"message\"\
      : \"No rows provided\"})\n    else:\n        rows_embedded = 0\n        batch_count\
      \ = 0\n        model_used = None\n        errors = []\n\n        for i in range(0,\
      \ len(rows), batch_size):\n            batch = rows[i:i + batch_size]\n    \
      \        batch_texts = [r.get('text', '') for r in batch]\n            batch_ids\
      \ = [str(r.get('id', '')) for r in batch]\n\n            # Skip empty texts\n\
      \            valid = [(id, text) for id, text in zip(batch_ids, batch_texts)\
      \ if text and text.strip()]\n            if not valid:\n                continue\n\
      \n            batch_ids, batch_texts = zip(*valid)\n\n            try:\n   \
      \             # Get embeddings via Agent (cost-tracked)\n                embed_result\
      \ = agent_embed_batch(\n                    texts=list(batch_texts),\n     \
      \               _session_id=_session_id,\n                    _caller_id=_caller_id,\n\
      \                    _cascade_id=_cascade_id,\n                    _cell_name=_cell_name,\n\
      \                )\n                embeddings = embed_result['embeddings']\n\
      \                model_used = embed_result['model']\n\n                # Prepare\
      \ documents for bulk ES insert\n                documents = []\n           \
      \     for row_id, text, embedding in zip(batch_ids, batch_texts, embeddings):\n\
      \                    documents.append({\n                        'source_table':\
      \ table_name,\n                        'source_id': row_id,\n              \
      \          'text': text,\n                        'embedding': embedding,\n\
      \                        'model': model_used,\n                        'metadata':\
      \ {'column_name': column_name}\n                    })\n\n                #\
      \ Bulk store in Elasticsearch\n                try:\n                    es_result\
      \ = elasticsearch_bulk_store_embeddings(\n                        documents=documents,\n\
      \                        index_name=index_name\n                    )\n    \
      \                rows_embedded += es_result.get('indexed', 0)\n            \
      \        if es_result.get('errors', 0) > 0:\n                        errors.append(f\"\
      batch {batch_count + 1}: {es_result['errors']} ES errors\")\n              \
      \  except Exception as e:\n                    errors.append(f\"batch {batch_count\
      \ + 1} ES store: {e}\")\n\n                batch_count += 1\n              \
      \  logger.info(f\"Embedded batch {batch_count}: {len(valid)} rows to Elasticsearch\"\
      )\n\n            except Exception as e:\n                logger.error(f\"Batch\
      \ {batch_count + 1} failed: {e}\")\n                errors.append(f\"batch {batch_count\
      \ + 1}: {e}\")\n\n        duration = time.time() - start_time\n\n        result\
      \ = json.dumps({\n            \"rows_embedded\": rows_embedded,\n          \
      \  \"rows_total\": len(rows),\n            \"batches\": batch_count,\n     \
      \       \"model\": model_used,\n            \"duration_seconds\": round(duration,\
      \ 2),\n            \"rows_per_second\": round(rows_embedded / duration, 1) if\
      \ duration > 0 else 0,\n            \"backend\": \"elasticsearch\",\n      \
      \      \"index\": index_name,\n            \"errors\": errors[:5] if errors\
      \ else None\n        })\n\nresult = result\n"
  context:
    enabled: false
