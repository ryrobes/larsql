cascade_id: embed_batch_elastic
internal: true

description: |
  Batch embed rows from a table, storing results in Elasticsearch.

  Uses Elasticsearch's dense_vector field for hybrid search (vector + keyword).
  This is preferred over ClickHouse when you want BM25 keyword matching combined
  with semantic similarity.

  Accepts a JSON array of {id, text} objects. Batches API calls (50 texts per call)
  and bulk indexes to Elasticsearch.

  SQL Usage:
    -- Embed from any table (DuckDB or ClickHouse)
    SELECT embed_batch_elastic(
      'products',
      'description',
      (SELECT to_json(list({'id': CAST(id AS VARCHAR), 'text': description})) FROM products)
    );

    -- With custom batch size and index name
    SELECT embed_batch_elastic('products', 'description', (SELECT ...), 100, 'my_index');

  Returns JSON stats:
    {
      "rows_embedded": 850,
      "batches": 17,
      "model": "qwen/qwen3-embedding-8b",
      "duration_seconds": 45.2,
      "backend": "elasticsearch"
    }

inputs_schema:
  table_name: "Source table name (for tracking)"
  column_name: "Column name (for metadata)"
  rows_json: "JSON array of {id, text} objects"
  batch_size: "Texts per API call (default: 50)"
  index_name: "Elasticsearch index (default: rvbbit_embeddings)"

sql_function:
  name: embed_batch_elastic
  description: "Batch embed rows and store in Elasticsearch for hybrid search"
  args:
    - name: table_name
      type: VARCHAR
      description: "Source table name (for tracking)"
    - name: column_name
      type: VARCHAR
      description: "Column name (for metadata)"
    - name: rows_json
      type: VARCHAR
      description: "JSON array of {id, text} objects"
    - name: batch_size
      type: INTEGER
      optional: true
      default: 50
      description: "Batch size (default 50)"
    - name: index_name
      type: VARCHAR
      optional: true
      default: "rvbbit_embeddings"
      description: "Elasticsearch index name"
  returns: VARCHAR
  shape: SCALAR
  cache: false

cells:
  - name: batch_embed_elastic
    tool: python_data
    inputs:
      code: |
        import time
        import json
        import logging
        from rvbbit.traits.embedding_storage import (
            agent_embed_batch,
            elasticsearch_bulk_store_embeddings
        )

        logger = logging.getLogger(__name__)
        start_time = time.time()

        # Get injected context for SQL Trail correlation
        _session_id = input.get('_session_id')
        _caller_id = input.get('_caller_id')
        _cascade_id = input.get('_cascade_id')
        _cell_name = input.get('_cell_name')

        table_name = input.get('table_name', 'unknown')
        column_name = input.get('column_name', 'unknown')
        rows_json = input.get('rows_json', '[]')
        batch_size = input.get('batch_size') or 50
        index_name = input.get('index_name') or 'rvbbit_embeddings'

        try:
            rows = json.loads(rows_json) if isinstance(rows_json, str) else rows_json
        except:
            result = json.dumps({"error": "Invalid JSON in rows_json"})
        else:
            if not rows:
                result = json.dumps({"rows_embedded": 0, "message": "No rows provided"})
            else:
                rows_embedded = 0
                batch_count = 0
                model_used = None
                errors = []

                for i in range(0, len(rows), batch_size):
                    batch = rows[i:i + batch_size]
                    batch_texts = [r.get('text', '') for r in batch]
                    batch_ids = [str(r.get('id', '')) for r in batch]

                    # Skip empty texts
                    valid = [(id, text) for id, text in zip(batch_ids, batch_texts) if text and text.strip()]
                    if not valid:
                        continue

                    batch_ids, batch_texts = zip(*valid)

                    try:
                        # Get embeddings via Agent (cost-tracked)
                        embed_result = agent_embed_batch(
                            texts=list(batch_texts),
                            _session_id=_session_id,
                            _caller_id=_caller_id,
                            _cascade_id=_cascade_id,
                            _cell_name=_cell_name,
                        )
                        embeddings = embed_result['embeddings']
                        model_used = embed_result['model']

                        # Prepare documents for bulk ES insert
                        documents = []
                        for row_id, text, embedding in zip(batch_ids, batch_texts, embeddings):
                            documents.append({
                                'source_table': table_name,
                                'source_id': row_id,
                                'text': text,
                                'embedding': embedding,
                                'model': model_used,
                                'metadata': {'column_name': column_name}
                            })

                        # Bulk store in Elasticsearch
                        try:
                            es_result = elasticsearch_bulk_store_embeddings(
                                documents=documents,
                                index_name=index_name
                            )
                            rows_embedded += es_result.get('indexed', 0)
                            if es_result.get('errors', 0) > 0:
                                errors.append(f"batch {batch_count + 1}: {es_result['errors']} ES errors")
                        except Exception as e:
                            errors.append(f"batch {batch_count + 1} ES store: {e}")

                        batch_count += 1
                        logger.info(f"Embedded batch {batch_count}: {len(valid)} rows to Elasticsearch")

                    except Exception as e:
                        logger.error(f"Batch {batch_count + 1} failed: {e}")
                        errors.append(f"batch {batch_count + 1}: {e}")

                duration = time.time() - start_time

                result = json.dumps({
                    "rows_embedded": rows_embedded,
                    "rows_total": len(rows),
                    "batches": batch_count,
                    "model": model_used,
                    "duration_seconds": round(duration, 2),
                    "rows_per_second": round(rows_embedded / duration, 1) if duration > 0 else 0,
                    "backend": "elasticsearch",
                    "index": index_name,
                    "errors": errors[:5] if errors else None
                })

        result = result
    context:
      enabled: false
