cascade_id: embed_column
internal: true

description: |
  Batch embed rows from a table, storing results in rvbbit_embeddings.

  Accepts a JSON array of {id, text} objects. Much more efficient than
  per-row embedding - batches API calls (50 texts per call).

  SQL Usage:
    -- Embed from any table (DuckDB or ClickHouse)
    SELECT embed_batch(
      'products',
      'description',
      (SELECT to_json(list({'id': CAST(id AS VARCHAR), 'text': description})) FROM products)
    );

    -- With batch size
    SELECT embed_batch('products', 'description', (SELECT ...), 100);

  Returns JSON stats:
    {
      "rows_embedded": 850,
      "batches": 17,
      "model": "qwen/qwen3-embedding-8b",
      "duration_seconds": 45.2
    }

inputs_schema:
  table_name: "Source table name (for tracking)"
  column_name: "Column name (for metadata)"
  rows_json: "JSON array of {id, text} objects"
  batch_size: "Texts per API call (default: 50)"

sql_function:
  name: embed_batch
  description: "Batch embed rows and store in rvbbit_embeddings"
  args:
    - name: table_name
      type: VARCHAR
      description: "Source table name (for tracking)"
    - name: column_name
      type: VARCHAR
      description: "Column name (for metadata)"
    - name: rows_json
      type: VARCHAR
      description: "JSON array of {id, text} objects"
    - name: batch_size
      type: INTEGER
      optional: true
      default: 50
      description: "Batch size (default 50)"
  returns: VARCHAR
  shape: SCALAR
  cache: false

cells:
  - name: batch_embed
    tool: python_data
    inputs:
      code: |
        import time
        import json
        import logging
        from rvbbit.traits.embedding_storage import agent_embed_batch, clickhouse_store_embedding

        logger = logging.getLogger(__name__)
        start_time = time.time()

        # Get injected context for SQL Trail correlation
        _session_id = input.get('_session_id')
        _caller_id = input.get('_caller_id')
        _cascade_id = input.get('_cascade_id')
        _cell_name = input.get('_cell_name')

        table_name = input.get('table_name', 'unknown')
        column_name = input.get('column_name', 'unknown')
        rows_json = input.get('rows_json', '[]')
        batch_size = input.get('batch_size') or 50

        try:
            rows = json.loads(rows_json) if isinstance(rows_json, str) else rows_json
        except:
            result = json.dumps({"error": "Invalid JSON in rows_json"})
        else:
            if not rows:
                result = json.dumps({"rows_embedded": 0, "message": "No rows provided"})
            else:
                rows_embedded = 0
                batch_count = 0
                model_used = None
                errors = []

                for i in range(0, len(rows), batch_size):
                    batch = rows[i:i + batch_size]
                    batch_texts = [r.get('text', '') for r in batch]
                    batch_ids = [str(r.get('id', '')) for r in batch]

                    # Skip empty texts
                    valid = [(id, text) for id, text in zip(batch_ids, batch_texts) if text and text.strip()]
                    if not valid:
                        continue

                    batch_ids, batch_texts = zip(*valid)

                    try:
                        embed_result = agent_embed_batch(
                            texts=list(batch_texts),
                            _session_id=_session_id,
                            _caller_id=_caller_id,
                            _cascade_id=_cascade_id,
                            _cell_name=_cell_name,
                        )
                        embeddings = embed_result['embeddings']
                        model_used = embed_result['model']

                        for row_id, text, embedding in zip(batch_ids, batch_texts, embeddings):
                            try:
                                clickhouse_store_embedding(
                                    source_table=table_name,
                                    source_id=row_id,
                                    text=text,
                                    embedding=embedding,
                                    model=model_used,
                                    metadata={'column_name': column_name}
                                )
                                rows_embedded += 1
                            except Exception as e:
                                errors.append(f"{row_id}: {e}")

                        batch_count += 1
                        logger.info(f"Embedded batch {batch_count}: {len(valid)} rows")

                    except Exception as e:
                        logger.error(f"Batch {batch_count + 1} failed: {e}")
                        errors.append(f"batch {batch_count + 1}: {e}")

                duration = time.time() - start_time

                result = json.dumps({
                    "rows_embedded": rows_embedded,
                    "rows_total": len(rows),
                    "batches": batch_count,
                    "model": model_used,
                    "duration_seconds": round(duration, 2),
                    "rows_per_second": round(rows_embedded / duration, 1) if duration > 0 else 0,
                    "errors": errors[:5] if errors else None
                })

        result = result
    context:
      enabled: false
