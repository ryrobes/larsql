cascade_id: embed_column
internal: true
description: "Batch embed rows from a table, storing results in rvbbit_embeddings.\n\
  \nAccepts a JSON array of {id, text} objects. Much more efficient than\nper-row\
  \ embedding - batches API calls (50 texts per call).\n\nSQL Usage:\n  -- Embed from\
  \ any table (DuckDB or ClickHouse)\n  SELECT embed_batch(\n    'products',\n   \
  \ 'description',\n    (SELECT to_json(list({'id': CAST(id AS VARCHAR), 'text': description}))\
  \ FROM products)\n  );\n\n  -- With batch size\n  SELECT embed_batch('products',\
  \ 'description', (SELECT ...), 100);\n\nReturns JSON stats:\n  {\n    \"rows_embedded\"\
  : 850,\n    \"batches\": 17,\n    \"model\": \"qwen/qwen3-embedding-8b\",\n    \"\
  duration_seconds\": 45.2\n  }\n"
inputs_schema:
  table_name: Source table name (for tracking)
  column_name: Column name (for metadata)
  rows_json: JSON array of {id, text} objects
  batch_size: 'Texts per API call (default: 50)'
sql_function:
  name: embed_batch
  description: Batch embed rows and store in rvbbit_embeddings
  args:
  - name: table_name
    type: VARCHAR
    description: Source table name (for tracking)
  - name: column_name
    type: VARCHAR
    description: Column name (for metadata)
  - name: rows_json
    type: VARCHAR
    description: JSON array of {id, text} objects
  - name: batch_size
    type: INTEGER
    optional: true
    default: 50
    description: Batch size (default 50)
  returns: VARCHAR
  shape: SCALAR
  cache: false
  test_cases:
  - sql: -- Requires embedding model
    skip: true
    description: Embed batch needs model
cells:
- name: batch_embed
  tool: python_data
  inputs:
    code: "import time\nimport json\nimport logging\nfrom rvbbit.skills.embedding_storage\
      \ import agent_embed_batch, clickhouse_store_embedding\n\nlogger = logging.getLogger(__name__)\n\
      start_time = time.time()\n\n# Get injected context for SQL Trail correlation\n\
      _session_id = input.get('_session_id')\n_caller_id = input.get('_caller_id')\n\
      _cascade_id = input.get('_cascade_id')\n_cell_name = input.get('_cell_name')\n\
      \ntable_name = input.get('table_name', 'unknown')\ncolumn_name = input.get('column_name',\
      \ 'unknown')\nrows_json = input.get('rows_json', '[]')\nbatch_size = input.get('batch_size')\
      \ or 50\n\ntry:\n    rows = json.loads(rows_json) if isinstance(rows_json, str)\
      \ else rows_json\nexcept:\n    result = json.dumps({\"error\": \"Invalid JSON\
      \ in rows_json\"})\nelse:\n    if not rows:\n        result = json.dumps({\"\
      rows_embedded\": 0, \"message\": \"No rows provided\"})\n    else:\n       \
      \ rows_embedded = 0\n        batch_count = 0\n        model_used = None\n  \
      \      errors = []\n\n        for i in range(0, len(rows), batch_size):\n  \
      \          batch = rows[i:i + batch_size]\n            batch_texts = [r.get('text',\
      \ '') for r in batch]\n            batch_ids = [str(r.get('id', '')) for r in\
      \ batch]\n\n            # Skip empty texts\n            valid = [(id, text)\
      \ for id, text in zip(batch_ids, batch_texts) if text and text.strip()]\n  \
      \          if not valid:\n                continue\n\n            batch_ids,\
      \ batch_texts = zip(*valid)\n\n            try:\n                embed_result\
      \ = agent_embed_batch(\n                    texts=list(batch_texts),\n     \
      \               _session_id=_session_id,\n                    _caller_id=_caller_id,\n\
      \                    _cascade_id=_cascade_id,\n                    _cell_name=_cell_name,\n\
      \                )\n                embeddings = embed_result['embeddings']\n\
      \                model_used = embed_result['model']\n\n                for row_id,\
      \ text, embedding in zip(batch_ids, batch_texts, embeddings):\n            \
      \        try:\n                        clickhouse_store_embedding(\n       \
      \                     source_table=table_name,\n                           \
      \ source_id=row_id,\n                            text=text,\n              \
      \              embedding=embedding,\n                            model=model_used,\n\
      \                            metadata={'column_name': column_name}\n       \
      \                 )\n                        rows_embedded += 1\n          \
      \          except Exception as e:\n                        errors.append(f\"\
      {row_id}: {e}\")\n\n                batch_count += 1\n                logger.info(f\"\
      Embedded batch {batch_count}: {len(valid)} rows\")\n\n            except Exception\
      \ as e:\n                logger.error(f\"Batch {batch_count + 1} failed: {e}\"\
      )\n                errors.append(f\"batch {batch_count + 1}: {e}\")\n\n    \
      \    duration = time.time() - start_time\n\n        result = json.dumps({\n\
      \            \"rows_embedded\": rows_embedded,\n            \"rows_total\":\
      \ len(rows),\n            \"batches\": batch_count,\n            \"model\":\
      \ model_used,\n            \"duration_seconds\": round(duration, 2),\n     \
      \       \"rows_per_second\": round(rows_embedded / duration, 1) if duration\
      \ > 0 else 0,\n            \"errors\": errors[:5] if errors else None\n    \
      \    })\n\nresult = result\n"
  context:
    enabled: false
