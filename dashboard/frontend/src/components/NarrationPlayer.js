import { useRef, useState, useEffect, useCallback } from 'react';

/**
 * NarrationPlayer - Browser-based audio player for narrator TTS
 *
 * Plays audio files generated by the backend 'say' tool using Web Audio API,
 * providing real-time amplitude data for visual animations.
 *
 * Features:
 * - Queue management (one audio at a time)
 * - Amplitude analysis via AnalyserNode
 * - Automatic cleanup after playback
 */
export function useNarrationPlayer({ onAmplitudeChange, onPlaybackStart, onPlaybackEnd }) {
  const audioContextRef = useRef(null);
  const analyserRef = useRef(null);
  const currentSourceRef = useRef(null);
  const animationFrameRef = useRef(null);
  const [isPlaying, setIsPlaying] = useState(false);
  const [queue, setQueue] = useState([]);

  // Initialize Audio Context and Analyser
  useEffect(() => {
    // Create AudioContext (lazy init to comply with browser autoplay policies)
    const getOrCreateAudioContext = () => {
      if (!audioContextRef.current) {
        const AudioContext = window.AudioContext || window.webkitAudioContext;
        audioContextRef.current = new AudioContext();

        // Create analyser for amplitude data
        analyserRef.current = audioContextRef.current.createAnalyser();
        analyserRef.current.fftSize = 256;
        analyserRef.current.connect(audioContextRef.current.destination);
      }
      return audioContextRef.current;
    };

    // Resume audio context on user interaction (browser autoplay policy)
    const resumeContext = () => {
      const ctx = getOrCreateAudioContext();
      if (ctx.state === 'suspended') {
        ctx.resume();
      }
    };

    // Attach event listeners for user interaction
    document.addEventListener('click', resumeContext);
    document.addEventListener('keydown', resumeContext);

    return () => {
      document.removeEventListener('click', resumeContext);
      document.removeEventListener('keydown', resumeContext);

      // Cleanup
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
    };
  }, []);

  // Amplitude animation loop
  const startAmplitudeMonitoring = useCallback(() => {
    if (!analyserRef.current) {
      console.warn('[NarrationPlayer] No analyser for amplitude monitoring');
      return;
    }

    //console.log('[NarrationPlayer] Starting amplitude monitoring...');
    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);

    const animate = () => {
      if (!isPlaying) {
        //console.log('[NarrationPlayer] Amplitude monitoring stopped (not playing)');
        return;
      }

      analyserRef.current.getByteFrequencyData(dataArray);

      // Calculate average amplitude (0-255 range)
      const sum = dataArray.reduce((a, b) => a + b, 0);
      const average = sum / dataArray.length;

      // Normalize to 0-1 range
      const normalizedAmplitude = average / 255;

      if (onAmplitudeChange) {
        onAmplitudeChange(normalizedAmplitude);
      }

      animationFrameRef.current = requestAnimationFrame(animate);
    };

    animate();
  }, [isPlaying, onAmplitudeChange]);

  // Play audio function
  const playAudio = useCallback(async (audioPath) => {
    //console.log('[NarrationPlayer] playAudio called with:', audioPath);

    try {
      const audioContext = audioContextRef.current;
      if (!audioContext) {
        console.error('[NarrationPlayer] AudioContext not initialized');
        return;
      }

      //console.log('[NarrationPlayer] AudioContext state:', audioContext.state);

      // Resume context if suspended
      if (audioContext.state === 'suspended') {
        //console.log('[NarrationPlayer] Resuming suspended AudioContext...');
        await audioContext.resume();
        //console.log('[NarrationPlayer] AudioContext resumed, new state:', audioContext.state);
      }

      // Extract filename from path
      const filename = audioPath.split('/').pop();
      const fetchUrl = `http://localhost:5001/api/audio/${filename}`;

      //console.log('[NarrationPlayer] Fetching audio from:', fetchUrl);

      // Fetch audio file from backend
      const response = await fetch(fetchUrl);

      if (!response.ok) {
        console.error('[NarrationPlayer] Failed to fetch audio:', response.status, response.statusText);
        return;
      }

      //console.log('[NarrationPlayer] Audio fetched successfully, decoding...');

      const arrayBuffer = await response.arrayBuffer();
      //console.log('[NarrationPlayer] ArrayBuffer size:', arrayBuffer.byteLength);

      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      //console.log('[NarrationPlayer] Audio decoded, duration:', audioBuffer.duration, 'seconds');

      // Create buffer source
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;

      // Connect to analyser for amplitude data
      source.connect(analyserRef.current);

      // Track current source for cleanup
      currentSourceRef.current = source;

      // Handle playback end
      source.onended = () => {
        setIsPlaying(false);
        if (animationFrameRef.current) {
          cancelAnimationFrame(animationFrameRef.current);
        }
        if (onAmplitudeChange) {
          onAmplitudeChange(0); // Reset amplitude
        }
        if (onPlaybackEnd) {
          onPlaybackEnd();
        }

        // Process queue
        setQueue(prevQueue => {
          const [, ...rest] = prevQueue;
          if (rest.length > 0) {
            // Play next in queue
            setTimeout(() => playAudio(rest[0]), 100);
          }
          return rest;
        });
      };

      // Start playback
      //console.log('[NarrationPlayer] Starting playback...');
      setIsPlaying(true);
      source.start(0);
      //console.log('[NarrationPlayer] Playback started!');

      if (onPlaybackStart) {
        onPlaybackStart();
      }

      //console.log(`[NarrationPlayer] âœ… Successfully playing audio: ${filename}`);

    } catch (error) {
      console.error('[NarrationPlayer] Playback error:', error);
      setIsPlaying(false);
      if (onPlaybackEnd) {
        onPlaybackEnd();
      }
    }
  }, [onAmplitudeChange, onPlaybackStart, onPlaybackEnd]);

  // Start amplitude monitoring when playback starts
  useEffect(() => {
    if (isPlaying) {
      startAmplitudeMonitoring();
    } else {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    }
  }, [isPlaying, startAmplitudeMonitoring]);

  // Public API: enqueue audio for playback
  const play = useCallback((audioPath) => {
    if (isPlaying) {
      // Add to queue
      setQueue(prevQueue => [...prevQueue, audioPath]);
      //console.log(`[NarrationPlayer] Audio queued (${queue.length + 1} in queue)`);
    } else {
      // Play immediately
      playAudio(audioPath);
    }
  }, [isPlaying, playAudio, queue.length]);

  // Public API: stop playback
  const stop = useCallback(() => {
    if (currentSourceRef.current) {
      try {
        currentSourceRef.current.stop();
        currentSourceRef.current.disconnect();
      } catch (e) {
        // Already stopped
      }
      currentSourceRef.current = null;
    }
    setIsPlaying(false);
    setQueue([]);
    if (onAmplitudeChange) {
      onAmplitudeChange(0);
    }
  }, [onAmplitudeChange]);

  return {
    play,
    stop,
    isPlaying,
    queueLength: queue.length
  };
}

/**
 * NarrationPlayer Component (optional - can use hook directly)
 */
export default function NarrationPlayer({ onAmplitudeChange, onPlaybackStart, onPlaybackEnd, playerRef }) {
  const player = useNarrationPlayer({ onAmplitudeChange, onPlaybackStart, onPlaybackEnd });

  // Expose player methods via ref
  useEffect(() => {
    if (playerRef) {
      playerRef.current = player;
    }
  }, [player, playerRef]);

  // This is a headless component - no UI rendering
  return null;
}
