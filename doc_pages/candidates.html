<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Candidates & Evaluation - RVBBIT Documentation</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="assets/docs.css">
</head>
<body>
  <div class="page">
    <header>
      <div class="container">
        <nav class="nav">
          <a href="index.html" class="brand"><span>RVBBIT</span></a>
          <div class="nav-links">
            <a href="index.html">Docs</a>
            <a href="https://github.com/rvbbit/rvbbit">GitHub</a>
          </div>
        </nav>
      </div>
    </header>

    <div class="container">
      <div class="docs-layout">
        <aside class="sidebar">
          <nav>
            <div class="sidebar-section">
              <h4 class="sidebar-title">Getting Started</h4>
              <ul class="sidebar-links">
                <li><a href="index.html">Overview</a></li>
                <li><a href="core-concepts.html">Core Concepts</a></li>
              </ul>
            </div>
            <div class="sidebar-section">
              <h4 class="sidebar-title">Cascade DSL</h4>
              <ul class="sidebar-links">
                <li><a href="cascade-dsl.html">DSL Reference</a></li>
                <li><a href="cell-types.html">Cell Types</a></li>
                <li><a href="validation.html">Validation (Wards)</a></li>
                <li><a href="context.html">Context Management</a></li>
              </ul>
            </div>
            <div class="sidebar-section">
              <h4 class="sidebar-title">Features</h4>
              <ul class="sidebar-links">
                <li><a href="semantic-sql.html">Semantic SQL</a></li>
                <li><a href="candidates.html" class="active">Candidates & Evaluation</a></li>
                <li><a href="tools.html">Tools (Traits)</a></li>
                <li><a href="mcp.html">MCP Integration</a></li>
              </ul>
            </div>
          </nav>
        </aside>

        <main class="docs-content">
          <h1>Candidates & Evaluation</h1>
          <p class="lead">
            Run multiple parallel attempts of a cell and select the best output using
            LLM evaluation, human judgment, cost-aware selection, or Pareto frontier analysis.
          </p>

          <div class="toc">
            <div class="toc-title">On This Page</div>
            <ul>
              <li><a href="#basics">Basic Candidates</a></li>
              <li><a href="#mutations">Mutations (Prompt Variations)</a></li>
              <li><a href="#evaluation">Evaluation Modes</a></li>
              <li><a href="#multi-model">Multi-Model Candidates</a></li>
              <li><a href="#reforge">Reforge (Refinement)</a></li>
              <li><a href="#human-eval">Human Evaluation</a></li>
              <li><a href="#execution">Execution Flow</a></li>
            </ul>
          </div>

          <h2 id="basics">Basic Candidates Configuration</h2>
          <p>
            Candidates enable you to run a cell multiple times in parallel and pick the best result.
            This is powerful for creative tasks, code generation, or benchmarking different models.
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Simple Candidates</span></div>
            <div class="code-block-content">
              <pre>- <span class="key">name</span>: generate_story
  <span class="key">instructions</span>: <span class="str">"Write an engaging story opening"</span>
  <span class="key">candidates</span>:
    <span class="key">factor</span>: <span class="num">5</span>                       <span class="cmt"># Run 5 parallel attempts</span>
    <span class="key">evaluator_instructions</span>: <span class="str">|
      Select the story opening with the most compelling hook.
      Consider creativity, clarity, and engagement.
    </span></pre>
            </div>
          </div>

          <h3>Dynamic Factor (Jinja2 Template)</h3>
          <p>
            The <code>factor</code> field can be a Jinja2 template, enabling dynamic
            parallelism based on prior cell outputs:
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Dynamic Factor</span></div>
            <div class="code-block-content">
              <pre>- <span class="key">name</span>: list_files
  <span class="key">tool</span>: python:os.listdir
  <span class="key">tool_inputs</span>:
    <span class="key">path</span>: <span class="str">"{{ input.directory }}"</span>

- <span class="key">name</span>: process_each_file
  <span class="key">instructions</span>: <span class="str">"Process file {{ sounding_index }}"</span>
  <span class="key">candidates</span>:
    <span class="key">factor</span>: <span class="str">"{{ outputs.list_files | length }}"</span>  <span class="cmt"># One per file!</span>
    <span class="key">evaluator_instructions</span>: <span class="str">"Select best processing result"</span></pre>
            </div>
          </div>

          <h3>Parallel Execution</h3>
          <p>
            Candidates execute in parallel using ThreadPoolExecutor:
          </p>

          <table>
            <thead>
              <tr><th>Field</th><th>Default</th><th>Description</th></tr>
            </thead>
            <tbody>
              <tr><td><code>max_parallel</code></td><td>3</td><td>Max concurrent executions</td></tr>
              <tr><td><code>factor</code></td><td>1</td><td>Total number of candidates</td></tr>
            </tbody>
          </table>

          <div class="info-box tip">
            <div class="info-box-title">Execution Isolation</div>
            <p>
              Each candidate runs in its own <code>RVBBITRunner</code> instance with isolated
              state/history. They share the same <code>session_id</code> for unified logging,
              but maintain independent execution contexts.
            </p>
          </div>

          <h2 id="mutations">Mutations (Prompt Variations)</h2>
          <p>
            Mutations generate different versions of your prompt to explore the solution space.
            Enable <strong>automatic learning</strong> from winning variations.
          </p>

          <h3>Three Mutation Modes</h3>

          <h4>1. Rewrite Mode (LLM-Driven)</h4>
          <p>
            An LLM rewrites your prompt using different strategies. Learns from previous
            winning rewrites!
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Rewrite Mode</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">factor</span>: <span class="num">5</span>
  <span class="key">mutate</span>: <span class="kw">true</span>
  <span class="key">mutation_mode</span>: rewrite  <span class="cmt"># Default</span>
  <span class="key">evaluator_instructions</span>: <span class="str">"Select best output"</span></pre>
            </div>
          </div>

          <p><strong>Built-in Rewrite Templates:</strong></p>
          <ul>
            <li>"Rewrite to be more specific and detailed..."</li>
            <li>"Rewrite to be more concise and focused..."</li>
            <li>"Rewrite to be more evocative and engaging..."</li>
            <li>"Rewrite to include specific constraints..."</li>
            <li>"Rewrite to encourage unique perspective..."</li>
            <li>"Rewrite to emphasize quality and polish..."</li>
          </ul>

          <div class="info-box">
            <div class="info-box-title">Species Hash Learning</div>
            <p>
              When using rewrite mode, RVBBIT computes a "species hash" from your cell config + input.
              It fetches the last 5 winning rewrites with the same hash and shows them as examples
              to the rewriter. This enables <strong>continuous learning</strong> from what works!
            </p>
          </div>

          <h4>2. Augment Mode (Prepended Text)</h4>
          <p>
            Prepend instruction fragments to the original prompt. Faster (no LLM call for mutation).
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Augment Mode</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">factor</span>: <span class="num">4</span>
  <span class="key">mutation_mode</span>: augment
  <span class="key">mutations</span>:  <span class="cmt"># Custom prepend templates</span>
    - <span class="str">"Be thorough and comprehensive. "</span>
    - <span class="str">"Be concise and direct. "</span>
    - <span class="str">"Focus on accuracy over speed. "</span></pre>
            </div>
          </div>

          <h4>3. Approach Mode (Thinking Strategies)</h4>
          <p>
            Append thinking strategy hints. Changes <em>how</em> the agent approaches the problem.
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Approach Mode</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">factor</span>: <span class="num">3</span>
  <span class="key">mutation_mode</span>: approach
  <span class="key">mutations</span>:
    - <span class="str">"Approach this from an unexpected angle."</span>
    - <span class="str">"Focus on the emotional element."</span>
    - <span class="str">"Think step-by-step systematically."</span></pre>
            </div>
          </div>

          <h3>Candidate Indexing</h3>
          <p>
            Each candidate has access to its index via Jinja2 variables:
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Candidate-Aware Prompts</span></div>
            <div class="code-block-content">
              <pre><span class="key">instructions</span>: <span class="str">|
  {% if sounding_index == 0 %}
    Generate a conservative, traditional approach.
  {% elif sounding_index == 1 %}
    Generate a creative, innovative approach.
  {% else %}
    Generate approach variant {{ sounding_index }}.
  {% endif %}

  Total attempts: {{ sounding_factor }}
</span></pre>
            </div>
          </div>

          <h2 id="evaluation">Evaluation Modes</h2>
          <p>
            RVBBIT supports four evaluation modes for selecting the winner.
          </p>

          <h3>Mode 1: LLM Evaluation (Default)</h3>
          <p>
            An LLM evaluator compares all candidates and selects the best.
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">LLM Evaluator</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">factor</span>: <span class="num">5</span>
  <span class="key">evaluator_instructions</span>: <span class="str">|
    Compare these story openings and select the best one.

    Criteria:
    - Engaging hook that captures attention
    - Clear writing style
    - Sets up compelling conflict

    Respond with ONLY the number of the best attempt and brief explanation.
  </span></pre>
            </div>
          </div>

          <p>
            The evaluator receives all candidate outputs formatted as:
          </p>

          <pre style="background: rgba(28, 246, 255, 0.1); padding: 12px; border-radius: 6px; font-family: 'JetBrains Mono', monospace; font-size: 0.8rem;">## Attempt 1
Result: [candidate output]

## Attempt 2
Result: [candidate output]
...</pre>

          <h3>Mode 2: Aggregate Mode (Combine All)</h3>
          <p>
            Instead of picking a winner, combine all candidate outputs into a single result.
            Perfect for brainstorming or gathering diverse perspectives.
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Aggregate Mode</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">factor</span>: <span class="num">4</span>
  <span class="key">mode</span>: aggregate  <span class="cmt"># Don't pick winner, combine all</span>
  <span class="key">aggregator_instructions</span>: <span class="str">|
    Synthesize these brainstorming outputs into a comprehensive list.
    Remove duplicates but preserve unique ideas from each attempt.
  </span>
  <span class="key">aggregator_model</span>: <span class="str">anthropic/claude-sonnet-4</span></pre>
            </div>
          </div>

          <h3>Mode 3: Human Evaluation (HITL)</h3>
          <p>
            Present all candidates to a human for selection. Captures preference data for training.
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Human Evaluation</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">factor</span>: <span class="num">5</span>
  <span class="key">evaluator</span>: human
  <span class="key">human_eval</span>:
    <span class="key">presentation</span>: side_by_side  <span class="cmt"># How to display</span>
    <span class="key">selection_mode</span>: pick_one    <span class="cmt"># How to select</span>
    <span class="key">show_metadata</span>: <span class="kw">true</span>         <span class="cmt"># Show cost, time, model</span>
    <span class="key">show_mutations</span>: <span class="kw">true</span>        <span class="cmt"># Show what variation was used</span>
    <span class="key">require_reasoning</span>: <span class="kw">true</span>     <span class="cmt"># Ask user to explain choice</span>
    <span class="key">capture_for_training</span>: <span class="kw">true</span>  <span class="cmt"># Log as preference data</span></pre>
            </div>
          </div>

          <h4>Presentation Modes</h4>
          <table>
            <thead>
              <tr><th>Mode</th><th>Description</th></tr>
            </thead>
            <tbody>
              <tr><td><code>side_by_side</code></td><td>Cards in a grid layout</td></tr>
              <tr><td><code>tabbed</code></td><td>Tab per candidate</td></tr>
              <tr><td><code>carousel</code></td><td>Swipe through candidates</td></tr>
              <tr><td><code>diff</code></td><td>Show differences highlighted</td></tr>
              <tr><td><code>tournament</code></td><td>Pairwise comparison brackets</td></tr>
            </tbody>
          </table>

          <h4>Selection Modes</h4>
          <table>
            <thead>
              <tr><th>Mode</th><th>Description</th></tr>
            </thead>
            <tbody>
              <tr><td><code>pick_one</code></td><td>Select single winner</td></tr>
              <tr><td><code>rank_all</code></td><td>Order all from best to worst</td></tr>
              <tr><td><code>rate_each</code></td><td>Give score to each (highest wins)</td></tr>
              <tr><td><code>tournament</code></td><td>Pairwise elimination</td></tr>
            </tbody>
          </table>

          <h3>Mode 4: Hybrid Evaluation</h3>
          <p>
            LLM prefilters to top N candidates, then human picks the final winner.
            Reduces human cognitive load.
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Hybrid Evaluation</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">factor</span>: <span class="num">10</span>
  <span class="key">evaluator</span>: hybrid
  <span class="key">llm_prefilter</span>: <span class="num">3</span>                <span class="cmt"># LLM picks top 3</span>
  <span class="key">llm_prefilter_instructions</span>: <span class="str">|
    Select the top 3 story openings based on creativity and engagement.
  </span>
  <span class="key">human_eval</span>:
    <span class="key">presentation</span>: side_by_side    <span class="cmt"># Human sees only top 3</span>
    <span class="key">selection_mode</span>: pick_one</pre>
            </div>
          </div>

          <h2 id="multi-model">Multi-Model Candidates</h2>
          <p>
            Distribute candidates across multiple models for benchmarking or
            quality/cost optimization.
          </p>

          <h3>Simple Model List</h3>
          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Model List</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">factor</span>: <span class="num">6</span>
  <span class="key">models</span>:
    - anthropic/claude-sonnet-4.5
    - x-ai/grok-4.1-fast
    - google/gemini-2.5-flash-lite
  <span class="key">model_strategy</span>: round_robin  <span class="cmt"># round_robin|random</span>
  <span class="key">evaluator_instructions</span>: <span class="str">"Select best output"</span></pre>
            </div>
          </div>

          <p><strong>Result:</strong> Models assigned in rotation: [claude, grok, gemini, claude, grok, gemini]</p>

          <h3>Per-Model Configuration</h3>
          <p>
            Specify how many candidates each model should generate:
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Per-Model Factors</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">models</span>:
    anthropic/claude-sonnet-4.5:
      <span class="key">factor</span>: <span class="num">2</span>
      <span class="key">temperature</span>: <span class="num">0.7</span>
    x-ai/grok-4.1-fast:
      <span class="key">factor</span>: <span class="num">2</span>
      <span class="key">temperature</span>: <span class="num">0.9</span>
    google/gemini-2.5-flash-lite:
      <span class="key">factor</span>: <span class="num">3</span>
  <span class="key">evaluator_instructions</span>: <span class="str">"Select best quality output"</span></pre>
            </div>
          </div>

          <p><strong>Total:</strong> 7 candidates (2+2+3)</p>

          <h3>Cost-Aware Evaluation</h3>
          <p>
            Balance quality and cost when selecting from multi-model candidates.
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Cost-Aware Evaluation</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">models</span>:
    - anthropic/claude-opus-4.5      <span class="cmt"># Expensive, high quality</span>
    - anthropic/claude-sonnet-4.5    <span class="cmt"># Balanced</span>
    - google/gemini-2.5-flash-lite   <span class="cmt"># Cheap, fast</span>
  <span class="key">cost_aware_evaluation</span>:
    <span class="key">enabled</span>: <span class="kw">true</span>
    <span class="key">quality_weight</span>: <span class="num">0.7</span>         <span class="cmt"># 70% quality</span>
    <span class="key">cost_weight</span>: <span class="num">0.3</span>            <span class="cmt"># 30% cost</span>
    <span class="key">show_costs_to_evaluator</span>: <span class="kw">true</span>
    <span class="key">cost_normalization</span>: min_max  <span class="cmt"># min_max|z_score|log_scale</span>
  <span class="key">evaluator_instructions</span>: <span class="str">|
    Select the best output considering BOTH quality and cost.
    If two outputs have similar quality, prefer the cheaper one.
  </span></pre>
            </div>
          </div>

          <p>
            The evaluator sees costs alongside outputs and balances the trade-off
            based on the configured weights.
          </p>

          <h3>Pareto Frontier Analysis</h3>
          <p>
            Identify non-dominated solutions (best cost/quality trade-offs) and select
            from the Pareto frontier.
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Pareto Frontier</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">models</span>:
    - anthropic/claude-opus-4.5
    - anthropic/claude-sonnet-4.5
    - google/gemini-2.5-flash-lite
    - x-ai/grok-4.1-fast
  <span class="key">pareto_frontier</span>:
    <span class="key">enabled</span>: <span class="kw">true</span>
    <span class="key">policy</span>: balanced             <span class="cmt"># prefer_cheap|prefer_quality|balanced|interactive</span>
    <span class="key">show_frontier</span>: <span class="kw">true</span>          <span class="cmt"># Log frontier data for viz</span>
    <span class="key">quality_metric</span>: evaluator_score
  <span class="key">evaluator_instructions</span>: <span class="str">"Rate each output on quality (0-100)"</span></pre>
            </div>
          </div>

          <h4>Pareto Policies</h4>
          <table>
            <thead>
              <tr><th>Policy</th><th>Selection Strategy</th></tr>
            </thead>
            <tbody>
              <tr><td><code>prefer_cheap</code></td><td>Minimum cost on frontier</td></tr>
              <tr><td><code>prefer_quality</code></td><td>Maximum quality on frontier</td></tr>
              <tr><td><code>balanced</code></td><td>Max (quality / cost) ratio on frontier</td></tr>
              <tr><td><code>interactive</code></td><td>Show frontier options, user selects</td></tr>
            </tbody>
          </table>

          <div class="info-box">
            <div class="info-box-title">Non-Dominated Solutions</div>
            <p>
              A solution is <strong>non-dominated</strong> if no other solution is both
              higher quality AND lower cost. The Pareto frontier contains all non-dominated
              solutions - the best possible cost/quality trade-offs.
            </p>
          </div>

          <h2 id="reforge">Reforge (Iterative Refinement)</h2>
          <p>
            After selecting a winner, iteratively refine it through multiple rounds
            of improvement. Enables <strong>depth-first exploration</strong> after
            breadth-first candidate generation.
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Reforge Configuration</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">factor</span>: <span class="num">5</span>
  <span class="key">evaluator_instructions</span>: <span class="str">"Select best initial draft"</span>
  <span class="key">reforge</span>:
    <span class="key">steps</span>: <span class="num">2</span>                    <span class="cmt"># Two refinement iterations</span>
    <span class="key">honing_prompt</span>: <span class="str">|
      Refine and improve the output.
      Focus on clarity and polish.
      Enhance weak areas.
    </span>
    <span class="key">factor_per_step</span>: <span class="num">3</span>         <span class="cmt"># 3 refinements per step</span>
    <span class="key">mutate</span>: <span class="kw">true</span>                <span class="cmt"># Apply mutations to refinements</span>
    <span class="key">evaluator_override</span>: <span class="str">|
      Select the most polished and refined version.
    </span>
    <span class="key">threshold</span>:                <span class="cmt"># Early stopping</span>
      <span class="key">validator</span>: <span class="str">"quality_check"</span>
      <span class="key">mode</span>: advisory</pre>
            </div>
          </div>

          <h3>Reforge Execution Flow</h3>
          <ol>
            <li><strong>Initial Candidates</strong>: Run 5 parallel attempts â†’ Select winner</li>
            <li><strong>Reforge Step 1</strong>:
              <ul>
                <li>Build refinement instructions: original + winner + honing_prompt</li>
                <li>Run 3 refinement attempts</li>
                <li>Evaluate â†’ Select new winner</li>
                <li>Check threshold validator (optional early stop)</li>
              </ul>
            </li>
            <li><strong>Reforge Step 2</strong>:
              <ul>
                <li>Refine step 1 winner</li>
                <li>Run 3 refinement attempts</li>
                <li>Evaluate â†’ Select final winner</li>
              </ul>
            </li>
            <li><strong>Return</strong>: Final refined output</li>
          </ol>

          <h3>Trace ID Hierarchy</h3>
          <p>All reforge executions are tracked with hierarchical IDs:</p>

          <pre style="background: rgba(28, 246, 255, 0.1); padding: 12px; border-radius: 6px; font-family: 'JetBrains Mono', monospace; font-size: 0.8rem;">session_123                    # Main session
session_123_sounding_0         # Initial candidate 0
session_123_sounding_1         # Initial candidate 1
...
session_123_reforge1_0         # Reforge step 1, attempt 0
session_123_reforge1_1         # Reforge step 1, attempt 1
session_123_reforge2_0         # Reforge step 2, attempt 0
...</pre>

          <h2 id="human-eval">Human Evaluation Deep Dive</h2>
          <p>
            Human evaluation captures rich preference data for improving evaluators
            and training models.
          </p>

          <h3>Complete Configuration</h3>
          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Full Human Eval Config</span></div>
            <div class="code-block-content">
              <pre><span class="key">human_eval</span>:
  <span class="cmt"># Presentation</span>
  <span class="key">presentation</span>: side_by_side
  <span class="key">selection_mode</span>: pick_one

  <span class="cmt"># Display options</span>
  <span class="key">show_metadata</span>: <span class="kw">true</span>         <span class="cmt"># Cost, tokens, time, model</span>
  <span class="key">show_mutations</span>: <span class="kw">true</span>        <span class="cmt"># What variation was applied</span>
  <span class="key">show_index</span>: <span class="kw">false</span>           <span class="cmt"># Hide to avoid bias</span>
  <span class="key">preview_render</span>: auto        <span class="cmt"># text|markdown|code|auto</span>
  <span class="key">max_preview_length</span>: <span class="num">500</span>    <span class="cmt"># Truncate long outputs</span>

  <span class="cmt"># Selection options</span>
  <span class="key">allow_reject_all</span>: <span class="kw">true</span>       <span class="cmt"># Option to reject all and retry</span>
  <span class="key">allow_tie</span>: <span class="kw">false</span>            <span class="cmt"># Can't select multiple as equal</span>
  <span class="key">require_reasoning</span>: <span class="kw">true</span>     <span class="cmt"># Must explain selection</span>

  <span class="cmt"># Timeout</span>
  <span class="key">timeout_seconds</span>: <span class="num">3600</span>       <span class="cmt"># 1 hour</span>
  <span class="key">on_timeout</span>: llm_fallback     <span class="cmt"># random|first|abort|llm_fallback</span>

  <span class="cmt"># Training data</span>
  <span class="key">capture_for_training</span>: <span class="kw">true</span>  <span class="cmt"># Log as preference data</span>
  <span class="key">capture_rejected_reasons</span>: <span class="kw">true</span>  <span class="cmt"># Why not the others?</span></pre>
            </div>
          </div>

          <h3>Preference Data Capture</h3>
          <p>
            Human preferences are logged to the <code>hotornot</code> system for:
          </p>

          <ul>
            <li><strong>Evaluator Validation</strong>: Does LLM judge match human preference?</li>
            <li><strong>Model Training</strong>: Train judge models from human feedback</li>
            <li><strong>Prompt Optimization</strong>: Identify winning mutation patterns</li>
          </ul>

          <h2>Pre-Evaluation Validation</h2>
          <p>
            Filter candidates before evaluation - only show valid outputs to the evaluator.
            Saves evaluator LLM calls on broken code or invalid formats.
          </p>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Pre-Eval Validator</span></div>
            <div class="code-block-content">
              <pre><span class="key">candidates</span>:
  <span class="key">factor</span>: <span class="num">5</span>
  <span class="key">validator</span>:  <span class="cmt"># Runs before evaluator</span>
    <span class="key">python</span>: <span class="str">|
      import ast
      try:
        ast.parse(content)
        result = {"valid": True, "reason": "Valid Python"}
      except SyntaxError as e:
        result = {"valid": False, "reason": f"Syntax error: {e}"}
    </span>
  <span class="key">evaluator_instructions</span>: <span class="str">|
    Select the best Python code (all candidates are syntactically valid).
  </span></pre>
            </div>
          </div>

          <p>
            If all candidates fail validation, falls back to evaluating all of them
            (no filtering applied).
          </p>

          <h2 id="execution">Complete Execution Flow</h2>

          <div class="code-block">
            <div class="code-block-header"><span class="code-block-lang">Candidate Execution Pipeline</span></div>
            <div class="code-block-content">
              <pre>Cell Start with candidates.factor=5
    â†“
<span class="num">1.</span> Resolve factor (handle Jinja2 templates)
    â†“
<span class="num">2.</span> Assign models (round_robin or per-model factors)
    â†“
<span class="num">3.</span> Filter models by context window
    â†“
<span class="num">4.</span> Pre-compute mutations (LLM rewrites if mode="rewrite")
    â†“
<span class="num">5.</span> ðŸ”± PARALLEL EXECUTION (ThreadPoolExecutor)
    â”œâ”€ Candidate 0 (model A, baseline or mutated)
    â”œâ”€ Candidate 1 (model B, mutated)
    â”œâ”€ Candidate 2 (model C, mutated)
    â””â”€ ... (up to max_parallel concurrent)
    â†“
<span class="num">6.</span> Collect results with metadata
    â”œâ”€ index, result, context, images
    â”œâ”€ trace_id, final_state
    â”œâ”€ mutation_applied, mutation_type
    â””â”€ model, cost, tokens
    â†“
<span class="num">7.</span> Pre-evaluation validation (optional)
    â””â”€ Filter valid candidates
    â†“
<span class="num">8.</span> SELECT EVALUATION MODE
    â”œâ”€ AGGREGATE â†’ Synthesize all outputs
    â”œâ”€ HUMAN â†’ Show UI, capture preference
    â”œâ”€ PARETO â†’ Compute frontier, select via policy
    â”œâ”€ COST-AWARE â†’ Balance quality (70%) + cost (30%)
    â””â”€ STANDARD â†’ LLM picks best
    â†“
<span class="num">9.</span> Extract winner_index
    â†“
<span class="num">10.</span> OPTIONAL: REFORGE (iterative refinement)
    â””â”€ For each step: refine winner N times â†’ evaluate
    â†“
Return winner result as cell output</pre>
            </div>
          </div>

          <h2>Best Practices</h2>

          <h3>When to Use Candidates</h3>
          <table>
            <thead>
              <tr><th>Use Case</th><th>Configuration</th></tr>
            </thead>
            <tbody>
              <tr><td><strong>Creative writing</strong></td><td>factor=5-10, mutate=true, mode=rewrite</td></tr>
              <tr><td><strong>Code generation</strong></td><td>Multi-model, validator filter, cost-aware</td></tr>
              <tr><td><strong>Brainstorming</strong></td><td>mode=aggregate, factor=5+</td></tr>
              <tr><td><strong>Model benchmarking</strong></td><td>Multi-model with human_eval</td></tr>
              <tr><td><strong>Quality assurance</strong></td><td>factor=3, reforge for polish</td></tr>
            </tbody>
          </table>

          <h3>Cost Optimization</h3>
          <ul>
            <li>Use <code>max_parallel</code> to control concurrency (costs add up!)</li>
            <li>Set <code>validator</code> to filter broken outputs before evaluation</li>
            <li>Use cheaper models in multi-model pool</li>
            <li>Enable <code>cost_aware_evaluation</code> for automatic cost/quality balance</li>
            <li>Use <code>pareto_frontier</code> to see all trade-off options</li>
          </ul>

          <div class="info-box warning">
            <div class="info-box-title">Cost Multiplier</div>
            <p>
              Running 5 candidates multiplies your cost by ~5x. Use candidates strategically
              for high-value outputs where quality matters (final reports, user-facing content,
              critical decisions). For routine operations, use single execution.
            </p>
          </div>

          <hr style="margin: 48px 0; border: none; border-top: 1px solid rgba(28, 246, 255, 0.2);">

          <h2>Next Steps</h2>
          <ul>
            <li><a href="validation.html">Validation</a> - Combine candidates with pre-eval validators</li>
            <li><a href="context.html">Context</a> - How context works with candidates</li>
            <li><a href="cascade-dsl.html">DSL Reference</a> - Complete CandidatesConfig fields</li>
          </ul>

        </main>
      </div>
    </div>

    <footer>
      <div class="container">
        <p>RVBBIT &copy; 2026 | <a href="https://github.com/rvbbit/rvbbit">GitHub</a></p>
      </div>
    </footer>
  </div>
</body>
</html>
