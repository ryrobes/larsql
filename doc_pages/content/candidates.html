<h1>Takes & Evaluation</h1>
<p class="lead">
  Run cells multiple times in parallel to find the best output.
  Use LLM evaluators, human evaluation, or aggregate all results.
</p>

<div class="toc">
  <div class="toc-title">On This Page</div>
  <ul>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#configuration">Configuration</a></li>
    <li><a href="#evaluators">Evaluators</a></li>
    <li><a href="#human-eval">Human Evaluation</a></li>
    <li><a href="#aggregate">Aggregate Mode</a></li>
    <li><a href="#reforge">Reforge</a></li>
  </ul>
</div>

<h2 id="overview">Overview</h2>

<p>
  The takes system (formerly "takes") allows you to:
</p>

<ul>
  <li>Run a cell N times in parallel</li>
  <li>Evaluate outputs to pick the best one</li>
  <li>Use different models per take</li>
  <li>Collect human feedback</li>
  <li>Aggregate results instead of picking a winner</li>
</ul>

<h2 id="configuration">Configuration</h2>

<div class="code-block">
  <div class="code-block-header">
    <span class="code-block-lang yaml">Basic Takes</span>
  </div>
  <div class="code-block-content">
    <pre>- <span class="key">name</span>: generate_copy
  <span class="key">instructions</span>: <span class="str">"Write marketing copy for {{ input.product }}"</span>
  <span class="key">takes</span>:
    <span class="key">factor</span>: <span class="num">5</span>  <span class="cmt"># Run 5 times</span>
    <span class="key">evaluator_instructions</span>: <span class="str">|
      Evaluate these marketing copies. Consider:
      - Clarity and persuasiveness
      - Brand alignment
      - Call-to-action effectiveness
      Pick the best one.
    </span></pre>
  </div>
</div>

<h3>Multi-Model Takes</h3>

<div class="code-block">
  <div class="code-block-header">
    <span class="code-block-lang yaml">Different Models</span>
  </div>
  <div class="code-block-content">
    <pre>- <span class="key">name</span>: analyze_data
  <span class="key">instructions</span>: <span class="str">"Analyze the sales data..."</span>
  <span class="key">takes</span>:
    <span class="key">models</span>:
      - <span class="str">anthropic/claude-sonnet-4</span>
      - <span class="str">openai/gpt-4o</span>
      - <span class="str">google/gemini-2.5-pro</span>
    <span class="key">evaluator_instructions</span>: <span class="str">|
      Compare the analyses. Pick the most thorough
      and accurate one.
    </span></pre>
  </div>
</div>

<h2 id="evaluators">Evaluators</h2>

<h3>LLM Evaluator (Default)</h3>

<p>
  An LLM reviews all outputs and picks the best. This is the default behavior when no
  <code>evaluator</code> is specified:
</p>

<div class="code-block">
  <div class="code-block-header">
    <span class="code-block-lang yaml">LLM Evaluator</span>
  </div>
  <div class="code-block-content">
    <pre><span class="key">takes</span>:
  <span class="key">factor</span>: <span class="num">5</span>
  <span class="cmt"># No 'evaluator' field = LLM evaluation (default)</span>
  <span class="key">evaluator_instructions</span>: <span class="str">|
    Evaluate on: accuracy, clarity, completeness.
    Explain your reasoning.
  </span></pre>
  </div>
</div>

<div class="info-box">
  <div class="info-box-title">Evaluator Options</div>
  <p>
    The <code>evaluator</code> field only accepts <code>human</code> or <code>hybrid</code>.
    Omit it entirely for LLM evaluation (the default). The evaluator uses the cell's model
    or system default model.
  </p>
</div>

<h3>Pre-Evaluation Validator</h3>

<p>
  Use a <code>validator</code> to filter takes <em>before</em> the LLM evaluator sees them.
  This is useful for filtering out takes that fail basic requirements (e.g., code that doesn't run):
</p>

<div class="code-block">
  <div class="code-block-header">
    <span class="code-block-lang yaml">Pre-Evaluation Validator</span>
  </div>
  <div class="code-block-content">
    <pre><span class="key">takes</span>:
  <span class="key">factor</span>: <span class="num">5</span>
  <span class="key">evaluator_instructions</span>: <span class="str">"Pick the best valid output"</span>
  <span class="key">validator</span>:                <span class="cmt"># Filters before LLM evaluation</span>
    <span class="key">python</span>: <span class="str">|
      # Return valid=True for takes that should be evaluated
      has_analysis = bool(output.get('analysis'))
      return {"valid": has_analysis, "reason": "Has analysis" if has_analysis else "Missing analysis"}
    </span></pre>
  </div>
</div>

<div class="info-box">
  <div class="info-box-title">Validator vs Evaluator</div>
  <p>
    The <code>validator</code> filters takes (pass/fail), then the LLM evaluator picks the
    winner from the remaining takes. Validators use the standard ward format:
    <code>{"valid": bool, "reason": str}</code>.
  </p>
</div>

<h2 id="human-eval">Human Evaluation</h2>

<p>
  Present takes to humans for selection:
</p>

<div class="code-block">
  <div class="code-block-header">
    <span class="code-block-lang yaml">Human Evaluation</span>
  </div>
  <div class="code-block-content">
    <pre><span class="key">takes</span>:
  <span class="key">factor</span>: <span class="num">5</span>
  <span class="key">evaluator</span>: human
  <span class="key">human_eval</span>:
    <span class="key">presentation</span>: side_by_side  <span class="cmt"># tabbed, carousel, diff, tournament</span>
    <span class="key">selection_mode</span>: pick_one    <span class="cmt"># rank_all, rate_each</span>
    <span class="key">show_metadata</span>: <span class="kw">true</span>
    <span class="key">require_reasoning</span>: <span class="kw">false</span>
    <span class="key">capture_for_training</span>: <span class="kw">true</span>  <span class="cmt"># Save choices for optimization</span>
    <span class="key">timeout_seconds</span>: <span class="num">3600</span>
    <span class="key">on_timeout</span>: llm_fallback</pre>
  </div>
</div>

<h3>Hybrid Evaluation</h3>

<p>
  LLM prefilters, human makes final choice:
</p>

<div class="code-block">
  <div class="code-block-header">
    <span class="code-block-lang yaml">Hybrid</span>
  </div>
  <div class="code-block-content">
    <pre><span class="key">takes</span>:
  <span class="key">factor</span>: <span class="num">10</span>
  <span class="key">evaluator</span>: hybrid
  <span class="key">llm_prefilter</span>: <span class="num">3</span>  <span class="cmt"># LLM picks top 3</span>
  <span class="key">human_eval</span>:
    <span class="key">presentation</span>: tabbed
    <span class="key">selection_mode</span>: pick_one</pre>
  </div>
</div>

<h2 id="aggregate">Aggregate Mode</h2>

<p>
  Instead of picking a winner, combine all outputs:
</p>

<div class="code-block">
  <div class="code-block-header">
    <span class="code-block-lang yaml">Aggregate</span>
  </div>
  <div class="code-block-content">
    <pre>- <span class="key">name</span>: brainstorm
  <span class="key">instructions</span>: <span class="str">"Generate ideas for {{ input.topic }}"</span>
  <span class="key">takes</span>:
    <span class="key">factor</span>: <span class="num">5</span>
    <span class="key">mode</span>: aggregate  <span class="cmt"># Combine all outputs</span>
    <span class="key">aggregator_instructions</span>: <span class="str">|
      Merge these idea lists. Remove duplicates,
      organize by theme, and rank by potential impact.
    </span></pre>
  </div>
</div>

<h2 id="reforge">Reforge</h2>

<p>
  Iteratively refine the winning take:
</p>

<div class="code-block">
  <div class="code-block-header">
    <span class="code-block-lang yaml">Reforge</span>
  </div>
  <div class="code-block-content">
    <pre><span class="key">takes</span>:
  <span class="key">factor</span>: <span class="num">3</span>
  <span class="key">evaluator_instructions</span>: <span class="str">"Pick the best draft"</span>
  <span class="key">reforge</span>:
    <span class="key">steps</span>: <span class="num">3</span>                   <span class="cmt"># Number of refinement iterations</span>
    <span class="key">honing_prompt</span>: <span class="str">|
      Improve this output. Address any weaknesses
      and enhance clarity and completeness.
    </span>
    <span class="key">factor_per_step</span>: <span class="num">2</span>         <span class="cmt"># Takes per reforge step</span>
    <span class="key">mutate</span>: <span class="kw">true</span>               <span class="cmt"># Apply variation strategies</span></pre>
  </div>
</div>

<h3>Reforge with Early Stopping</h3>

<p>
  Use a threshold (ward-like validator) for early stopping:
</p>

<div class="code-block">
  <div class="code-block-header">
    <span class="code-block-lang yaml">Reforge with Threshold</span>
  </div>
  <div class="code-block-content">
    <pre><span class="key">takes</span>:
  <span class="key">factor</span>: <span class="num">3</span>
  <span class="key">evaluator_instructions</span>: <span class="str">"Pick the best draft"</span>
  <span class="key">reforge</span>:
    <span class="key">steps</span>: <span class="num">5</span>
    <span class="key">honing_prompt</span>: <span class="str">"Refine further..."</span>
    <span class="key">threshold</span>:                 <span class="cmt"># Early stopping validator</span>
      <span class="key">validator</span>:
        <span class="key">python</span>: <span class="str">|
          score = len(output.get('analysis', '')) / 1000
          return {"valid": score >= 0.8, "reason": f"Score: {score}"}
        </span>
      <span class="key">mode</span>: blocking</pre>
  </div>
</div>

<div class="info-box">
  <div class="info-box-title">Cost Considerations</div>
  <p>
    Takes multiply token usage. A <code>factor: 5</code> with reforge can
    use 5-15x the tokens of a single run. Use wisely for high-value tasks.
  </p>
</div>

<hr>

<h2>Next: Tools (Skills)</h2>
<p>
  Learn about the tool system: <a href="#tools" data-link>Tools (Skills)</a>.
</p>
