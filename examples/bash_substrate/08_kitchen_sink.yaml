cascade_id: "bash_kitchen_sink"
description: "Showcase all bash_data features: persistence, data flow, mixed substrates"

phases:
  # Phase 1: Setup persistent environment (bash)
  - name: init_environment
    tool: bash_data
    inputs:
      script: |
        # Export config (persists across phases)
        export PROJECT="analytics_pipeline"
        export ENV="production"

        # Setup workspace
        mkdir -p data output logs
        cd data

        # Define helper functions (persist)
        function log() {
          echo "[$(date '+%H:%M:%S')] $1" >&2
        }

        function timestamp() {
          date '+%Y-%m-%d %H:%M:%S'
        }

        log "Environment initialized: $PROJECT ($ENV)"

        # Output confirmation
        echo "component,status"
        echo "environment,ready"
      output_format: csv
    handoffs: [generate_data]

  # Phase 2: Generate mock data with bash
  - name: generate_data
    tool: bash_data
    inputs:
      script: |
        log "Generating sample data for $PROJECT..."

        # Generate CSV data
        echo "id,user,action,timestamp"
        echo "1,alice,login,$(timestamp)"
        sleep 0.01
        echo "2,bob,view_dashboard,$(timestamp)"
        sleep 0.01
        echo "3,alice,run_query,$(timestamp)"
        sleep 0.01
        echo "4,charlie,login,$(timestamp)"
        sleep 0.01
        echo "5,bob,logout,$(timestamp)"

        log "Generated 5 events"
      output_format: csv
    handoffs: [transform_bash]

  # Phase 3: Transform with bash (using stdin)
  - name: transform_bash
    tool: bash_data
    inputs:
      script: |
        log "Transforming data from $PROJECT..."

        # Read CSV from stdin, add derived fields
        head -n 1  # Keep header
        echo "user,action,hour,env"  # New header

        tail -n +2 | awk -F',' -v env="$ENV" '{
          # Extract hour from timestamp
          split($4, parts, " ")
          split(parts[2], time_parts, ":")
          hour = time_parts[1]

          # Output: user, action, hour, env
          print $2 "," $3 "," hour "," env
        }'

        log "Transform complete"
      output_format: csv
    handoffs: [analyze_sql]

  # Phase 4: Analyze with SQL
  - name: analyze_sql
    tool: sql_data
    inputs:
      query: |
        SELECT
          user,
          COUNT(*) as action_count,
          COUNT(DISTINCT action) as unique_actions
        FROM _transform_bash
        GROUP BY user
        ORDER BY action_count DESC
    handoffs: [enrich_python]

  # Phase 5: Enrich with Python
  - name: enrich_python
    tool: python_data
    inputs:
      code: |
        df = data.analyze_sql.copy()
        df['tier'] = df['action_count'].apply(lambda x: 'power_user' if x >= 2 else 'casual')
        df['score'] = df['action_count'] * 10 + df['unique_actions'] * 5
        result = df
    handoffs: [final_report_bash]

  # Phase 6: Generate final report with bash (using persistent functions)
  - name: final_report_bash
    tool: bash_data
    inputs:
      script: |
        log "Generating final report for $PROJECT..."
        log "Environment: $ENV"
        log "Working directory: $(pwd)"

        # Query the enriched data via DuckDB
        echo "=== Top Users Report ===" >&2
        duckdb $SESSION_DB "
          SELECT user, tier, score
          FROM _enrich_python
          ORDER BY score DESC
        " -box >&2

        # Output summary as CSV
        echo "metric,value"
        echo "total_users,$(duckdb $SESSION_DB 'SELECT COUNT(*) FROM _enrich_python' -csv | tail -1)"
        echo "power_users,$(duckdb $SESSION_DB \"SELECT COUNT(*) FROM _enrich_python WHERE tier='power_user'\" -csv | tail -1)"
        echo "project,$PROJECT"
        echo "environment,$ENV"

        log "Report complete!"
      output_format: csv
