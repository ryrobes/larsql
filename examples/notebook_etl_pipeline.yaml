cascade_id: etl_pipeline_demo
description: ETL pipeline demonstrating data extraction, transformation across languages,
  and loading back to SQL for querying.
inputs_schema:
  min_score: 'Minimum score threshold for filtering (default: 50)'
cells:
- name: extract
  tool: sql_data
  inputs:
    query: "-- Extract: Generate user activity data\nSELECT * FROM (\n  VALUES\n \
      \   (1, 'alice', 'login', '2024-03-01 09:00:00', 100),\n    (1, 'alice', 'purchase',\
      \ '2024-03-01 09:15:00', 250),\n    (1, 'alice', 'logout', '2024-03-01 10:00:00',\
      \ 50),\n    (2, 'bob', 'login', '2024-03-01 10:30:00', 100),\n    (2, 'bob',\
      \ 'view', '2024-03-01 10:35:00', 30),\n    (2, 'bob', 'view', '2024-03-01 10:40:00',\
      \ 30),\n    (2, 'bob', 'purchase', '2024-03-01 11:00:00', 300),\n    (3, 'charlie',\
      \ 'login', '2024-03-01 08:00:00', 100),\n    (3, 'charlie', 'view', '2024-03-01\
      \ 08:05:00', 30),\n    (3, 'charlie', 'logout', '2024-03-01 08:30:00', 50),\n\
      \    (4, 'diana', 'login', '2024-03-01 14:00:00', 100),\n    (4, 'diana', 'purchase',\
      \ '2024-03-01 14:10:00', 500),\n    (4, 'diana', 'purchase', '2024-03-01 14:20:00',\
      \ 350),\n    (4, 'diana', 'logout', '2024-03-01 15:00:00', 50)\n) AS t(user_id,\
      \ username, action, timestamp, points)\n"
  handoffs:
  - transform_python
- name: transform_python
  tool: python_data
  inputs:
    code: "# Transform: Calculate user engagement metrics\nimport pandas as pd\n\n\
      df = data.extract.copy()\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\
      \n# Calculate per-user metrics\nuser_metrics = df.groupby(['user_id', 'username']).agg(\n\
      \    total_actions=('action', 'count'),\n    total_points=('points', 'sum'),\n\
      \    purchases=('action', lambda x: (x == 'purchase').sum()),\n    first_action=('timestamp',\
      \ 'min'),\n    last_action=('timestamp', 'max')\n).reset_index()\n\n# Calculate\
      \ session duration in minutes\nuser_metrics['session_minutes'] = (\n    (user_metrics['last_action']\
      \ - user_metrics['first_action'])\n    .dt.total_seconds() / 60\n).round(1)\n\
      \n# Convert timestamps to strings for JSON serialization\nuser_metrics['first_action']\
      \ = user_metrics['first_action'].astype(str)\nuser_metrics['last_action'] =\
      \ user_metrics['last_action'].astype(str)\n\nresult = user_metrics\n"
  handoffs:
  - enrich_js
- name: enrich_js
  tool: js_data
  inputs:
    code: "// Enrich: Add engagement scores and tiers\nconst users = data.transform_python;\n\
      const minScore = parseInt(input.min_score) || 50;\n\nconst enriched = users.map(user\
      \ => {\n  // Calculate engagement score (weighted formula)\n  const engagementScore\
      \ = Math.round(\n    (user.purchases * 50) +\n    (user.total_actions * 10)\
      \ +\n    (user.session_minutes * 2)\n  );\n\n  // Assign tier based on score\n\
      \  let tier, tierColor;\n  if (engagementScore >= 200) {\n    tier = 'Gold';\
      \ tierColor = '#FFD700';\n  } else if (engagementScore >= 100) {\n    tier =\
      \ 'Silver'; tierColor = '#C0C0C0';\n  } else {\n    tier = 'Bronze'; tierColor\
      \ = '#CD7F32';\n  }\n\n  return {\n    ...user,\n    engagement_score: engagementScore,\n\
      \    tier,\n    tier_color: tierColor,\n    meets_threshold: engagementScore\
      \ >= minScore\n  };\n});\n\n// Sort by engagement score descending\nenriched.sort((a,\
      \ b) => b.engagement_score - a.engagement_score);\nresult = enriched;\n"
  handoffs:
  - load_summary
- name: load_summary
  tool: sql_data
  inputs:
    query: "-- Load: Final summary with tier breakdown\nWITH enriched AS (\n  SELECT\
      \ * FROM _enrich_js\n),\ntier_stats AS (\n  SELECT\n    tier,\n    COUNT(*)\
      \ as user_count,\n    AVG(engagement_score) as avg_score,\n    SUM(purchases)\
      \ as total_purchases,\n    SUM(total_points) as total_points\n  FROM enriched\n\
      \  GROUP BY tier\n)\nSELECT\n  tier,\n  user_count,\n  ROUND(avg_score, 1) as\
      \ avg_engagement_score,\n  total_purchases,\n  total_points\nFROM tier_stats\n\
      ORDER BY avg_score DESC\n"
