cascade_id: sql_chart_gen_analysis_full_evo

description: Auto-discovers interesting data questions, executes SQL queries, creates charts, iterates
  on them, and presents findings.

inputs_schema:
  focus_area: (Optional) Area of interest to focus the analysis on, e.g. 'geographic trends', 'time patterns',
    'outliers'

memory: sql_rag_search

phases:

  - handoffs:
      - discover_schema

    instructions: "Your goal is to explore the available data and come up with an INTERESTING analytical\
      \ question to investigate.\n\n{% if input.focus_area %}Focus area hint: {{ input.focus_area }}{%\
      \ endif %}\n\n**Step 1: Discover what data is available**\nUse `list_sql_connections` to see available\
      \ data sources.\nUse `sql_search` with broad terms to discover tables (e.g., 'sales', 'users', 'events',\
      \ or just browse what's there).\n\n**Step 2: Sample the data (DO NOT read everything!)**\nOnce you\
      \ find interesting tables, use `sql_query` to get SMALL SAMPLES:\n- `SELECT * FROM connection.table\
      \ LIMIT 5` to see structure\n- `SELECT COUNT(*) FROM connection.table` for size\n- `SELECT DISTINCT\
      \ column FROM connection.table LIMIT 20` to see categorical values\n- `SELECT column, COUNT(*) FROM\
      \ connection.table GROUP BY column LIMIT 10` for distributions\n\n**Step 3: Formulate an interesting\
      \ question**\nBased on what you discovered, come up with a question that:\n- Is answerable with\
      \ the data available\n- Would produce an interesting visualization\n- Reveals patterns, trends,\
      \ comparisons, or anomalies\n- Is specific enough to be actionable\n\nGood questions often involve:\n\
      - Comparisons (\"Which X has the highest Y?\")\n- Trends over time (\"How has X changed over time?\"\
      )\n- Distributions (\"What is the distribution of X across Y?\")\n- Correlations (\"Is there a relationship\
      \ between X and Y?\")\n- Outliers (\"What are the unusual cases of X?\")\n\n**IMPORTANT**: Use `set_state`\
      \ to save your chosen question:\n```\nset_state(key=\"question\", value=\"Your specific analytical\
      \ question here\")\n```\n\nEnd your response by clearly stating the question you've chosen to investigate."

    name: discover_question

    rules:
      max_turns: 3

    soundings:
      evaluator_instructions: "Evaluate question discovery attempts based on:\n\n1. **Question Quality**\
        \ (40%): Is the question interesting, specific, and likely to produce insightful analysis?\n2.\
        \ **Data Fit** (30%): Is the question well-suited to the available data? Can it actually be answered?\n\
        3. **Visualization Potential** (20%): Would answering this question produce a compelling chart?\n\
        4. **Exploration Quality** (10%): Did they efficiently sample the data without over-reading?\n\
        \nSelect the attempt that:\n- Formulated the MOST interesting and insightful question\n- Demonstrated\
        \ efficient data exploration (sampling, not bulk reading)\n- Chose a question that's specific\
        \ enough to be actionable\n- Successfully saved the question to state\n\nPrefer questions about\
        \ trends, comparisons, distributions, or anomalies over simple counts or lists."
      factor: 7
      model_strategy: round-robin
      models:
        - anthropic/claude-sonnet-4.5
        - x-ai/grok-4.1-fast
        - amazon/nova-premier-v1
        - openai/gpt-5.1
        - openai/gpt-4.1
        - anthropic/claude-opus-4.5
        - google/gemini-3-pro-preview
      mutate: yes
      mutation_mode: rewrite

    tackle:
      - sql_search
      - list_sql_connections
      - sql_rag_search
      - sql_query
      - set_state

  - context:
      from:
        - previous
    handoffs:
      - write_query
    instructions: "The question to investigate: {{ state.question }}\n\nNow do a focused schema discovery\
      \ to find the exact tables and columns needed.\n\nUse sql_search to find relevant tables that contain\
      \ data to answer this question.\n\nSearch for tables using natural language - the tool will return\
      \ schema information including:\n- Column names and types\n- Value distributions for categorical\
      \ columns\n- Sample rows\n- Row counts\n\nAnalyze the results and identify which table(s) would\
      \ be most useful. \nNote: the search data is ephemeral, so if you need access to it later please\
      \ include important parts in your reply."
    name: discover_schema
    soundings:
      evaluator_instructions: "Evaluate schema discovery attempts based on:\n\n1. **Relevance** (40%):\
        \ Did they find tables that directly answer the user's question?\n2. **Thoroughness** (30%): Did\
        \ they examine multiple relevant tables and understand the schema structure?\n3. **Data Understanding**\
        \ (20%): Do they demonstrate understanding of column types, value distributions, and data quality?\n\
        4. **Actionability** (10%): Did they provide clear next steps for query writing, including specific\
        \ column names and table references?\n\nSelect the attempt that:\n- Found the MOST relevant table(s)\
        \ for the question\n- Demonstrated deepest understanding of the data structure\n- Provided most\
        \ useful information for the next phase (query writing)\n- Avoided irrelevant tables while being\
        \ thorough\n\nPrefer attempts that balance breadth (checking multiple options) with depth (understanding\
        \ what's useful)."
      factor: 7
      model_strategy: round-robin
      models:
        - anthropic/claude-sonnet-4.5
        - x-ai/grok-4.1-fast
        - amazon/nova-premier-v1
        - openai/gpt-5.1
        - openai/gpt-4.1
        - anthropic/claude-opus-4.5
        - google/gemini-3-pro-preview
      mutate: yes
      mutation_mode: rewrite
    tackle:
      - sql_search
      - list_sql_connections
      - sql_rag_search

  - context:
      from:
        - previous
    handoffs:
      - analyze_results
    instructions: "Based on the schema information from the previous phase, write a SQL query to answer:\
      \ {{ state.question }}\n\nUse the sql_query tool to execute your query.\n\nIMPORTANT Query Syntax:\n\
      - CSV folders: SELECT * FROM connection_name.table_name\n- PostgreSQL/MySQL: SELECT * FROM connection_name.schema.table\n\
      - Example: SELECT * FROM csv_files.bigfoot_sightings WHERE state = 'California'\n\nThe schema info\
      \ from the previous phase shows you:\n- Exact column names\n- Data types\n- Sample values (use these\
      \ to understand the data format)\n- Value distributions (shows you what values exist in categorical\
      \ columns)\n\nWrite a clear, efficient query based on this information."
    name: write_query
    soundings:
      evaluator_instructions: "Evaluate SQL query attempts based on:\n\n1. **Correctness** (40%): Does\
        \ the query execute without errors and return results?\n2. **Accuracy** (30%): Do the results\
        \ actually answer the user's question?\n3. **Efficiency** (15%): Is the query optimized (proper\
        \ filters, aggregations, no unnecessary data)?\n4. **Completeness** (15%): Does it return all\
        \ relevant data needed to answer the question?\n\nSelect the attempt that:\n- Executed successfully\
        \ (queries with errors are automatic losers)\n- Returns the MOST relevant data for the user's\
        \ question\n- Uses proper SQL syntax for the connection type\n- Applies appropriate filters, aggregations,\
        \ or joins\n- Returns a reasonable amount of data (not too little, not too much)\n\nCRITICAL:\
        \ Syntax errors or failed queries should be heavily penalized. A working query that's slightly\
        \ less optimal is better than a perfect-looking query that fails.\n\nPrefer queries that demonstrate\
        \ understanding of the schema and make smart decisions about what columns/rows to include."
      factor: 7
      model_strategy: round-robin
      models:
        - anthropic/claude-sonnet-4.5
        - x-ai/grok-4.1-fast
        - amazon/nova-premier-v1
        - openai/gpt-5.1
        - openai/gpt-4.1
        - anthropic/claude-opus-4.5
        - google/gemini-3-pro-preview
      mutate: yes
      mutation_mode: rewrite
    tackle:
      - sql_query
      - sql_rag_search

  - context:
      from:
        - previous
    handoffs:
      - create_initial_chart
    instructions: "Analyze the query results from the previous phase and provide a clear answer to: {{\
      \ state.question }}\n\nSummarize:\n- Key findings from the data\n- Any interesting patterns or insights\n\
      - Direct answer to the user's question\n\nBe specific and reference actual numbers from the results."
    name: analyze_results
    soundings:
      evaluator_instructions: "Evaluate data analysis attempts based on:\n\n1. **Accuracy** (35%): Does\
        \ the analysis correctly interpret the query results?\n2. **Insight Quality** (30%): Does it identify\
        \ meaningful patterns, trends, or anomalies?\n3. **Specificity** (20%): Does it reference actual\
        \ numbers, percentages, or concrete evidence from the data?\n4. **Clarity** (15%): Is the answer\
        \ clear, well-structured, and directly addresses the user's question?\n\nSelect the attempt that:\n\
        - Provides the MOST insightful interpretation of the data\n- Directly and clearly answers the\
        \ user's original question\n- Supports claims with specific numbers/evidence from the results\n\
        - Identifies interesting patterns beyond just the obvious\n- Avoids vague or generic statements\n\
        - Demonstrates quantitative thinking (percentages, comparisons, trends)\n\nPrefer analyses that\
        \ go beyond surface-level observations and provide actionable insights. An analysis that says\
        \ \"Sales increased by 47% in Q3, primarily driven by the Northeast region (23% growth)\" is better\
        \ than \"Sales went up.\"\n\nPenalize attempts that:\n- Make claims not supported by the data\n\
        - Are too vague or generic\n- Miss obvious patterns in the results\n- Don't directly answer the\
        \ user's question.\n\n IMPORTANT: Include the SQL query and a small sample of the data in your\
        \ response - enough context so a chart could be constructed from the data."
      factor: 7
      model_strategy: round-robin
      models:
        - anthropic/claude-sonnet-4.5
        - x-ai/grok-4.1-fast
        - amazon/nova-premier-v1
        - openai/gpt-5.1
        - openai/gpt-4.1
        - anthropic/claude-opus-4.5
        - google/gemini-3-pro-preview
      mutate: yes
      mutation_mode: rewrite
    tackle: []

  - context:
      from:
        - previous
    handoffs:
      - say_summary_summarize
    instructions: "Create a chart based on this data to help illustrate the findings. \n\nUse the create_vega_lite\
      \ or create_plotly tool. After creating it, the image will be automatically injected for you to\
      \ see.\n\nOnce you see the chart image, analyze it against the requirements and describe:\n1. What\
      \ you observe visually\n2. How well it meets the requirements\n3. What specific improvements are\
      \ needed - does it help to answer the users question? If mutiple compound charts would help visualize,\
      \ go for that. For styling use dark mode and give it some flair and personality."
    name: create_initial_chart
    rules:
      max_turns: 1
    soundings:
      evaluator_instructions: "Evaluate based on:\n1. Accuracy of visual observations about the chart\n\
        2. Depth of analysis against requirements\n3. Specificity of improvement suggestions\n\nSelect\
        \ the analysis that is most detailed and actionable. Must have a successfully generated chart\
        \ images to be valid."
      factor: 7
      model_strategy: round-robin
      models:
        - anthropic/claude-sonnet-4.5
        - x-ai/grok-4.1-fast
        - amazon/nova-premier-v1
        - openai/gpt-5.1
        - openai/gpt-4.1
        - anthropic/claude-opus-4.5
        - google/gemini-3-pro-preview
      mutate: yes
      mutation_mode: rewrite
      reforge:
        evaluator_override: |-
          Evaluate refinements on:
          1. Specificity (exact values, not vague suggestions)
          2. Visual awareness (referencing actual chart elements)
          3. Implementation feasibility
          4. Completeness of improvements

          Pick the most production-ready, implementable analysis.
        factor_per_step: 2
        honing_prompt: "You previously analyzed this chart. Looking at the chart image again, provide\
          \ even MORE specific improvements:\n\n- Exact color codes for better contrast (hex values)\n\
          - Precise font sizes for labels\n- Specific axis range adjustments\n- Data point annotation\
          \ placement\n- Are things being overlapped or clipped? \nLegend positioning coordinates\n\n\
          Be EXTREMELY specific and actionable. Reference exact elements you see in the image."
        mutate: yes
        steps: 2
    tackle:
      - sql_query
      - create_vega_lite
      - create_plotly
  - context:
      from:
        - all
    instructions: Use the `say` tool to speak to the user and give a VERY short summary of the findings
      and chart. Persona is a sassy, foul mouthed, street kid turned corpo. Yet a beautiful vixen, none
      the less.
    model: anthropic/claude-sonnet-4.5
    name: say_summary_summarize
    rules:
      max_turns: 1
    tackle:
      - say
