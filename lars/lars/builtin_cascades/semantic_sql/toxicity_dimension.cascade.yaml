cascade_id: toxicity_dimension
internal: true
description: 'Toxicity/civility analyzer for social media content. Assesses how toxic,

  hostile, or uncivil text content is.


  Can focus on specific types: harassment, hate speech, personal attacks,

  profanity, threats, etc.


  This is a DIMENSION-shaped function for use in GROUP BY clauses.

  '
inputs_schema:
  texts: JSON array of all text values to analyze
  focus: Optional focus area (harassment, hate_speech, profanity, threats, etc.)
  num_levels: Number of toxicity levels (default 5)
sql_function:
  name: toxicity
  description: 'Assess toxicity/civility level of text content.

    Optional focus parameter targets specific toxicity types.

    '
  shape: DIMENSION
  mode: mapping
  args:
  - name: text
    type: VARCHAR
    role: dimension_source
  - name: focus
    type: VARCHAR
    default: null
    description: Focus on specific type (harassment, hate_speech, threats, profanity)
  - name: num_levels
    type: INTEGER
    default: 5
  returns: VARCHAR
  cache: true
  test_cases:
  - sql: SELECT toxicity('Thank you for your help!')
    expect:
      type: regex
      pattern: .+
    description: Toxicity assessed
cells:
- name: assess_toxicity
  model: google/gemini-2.5-flash-lite
  instructions: "Assess the TOXICITY/CIVILITY level of these {{ input.texts | length\
    \ }} texts.\n\n{% if input.focus %}\nFOCUS SPECIFICALLY ON: {{ input.focus }}\n\
    {% if input.focus == 'harassment' %}\nLook for: targeted attacks, bullying, intimidation,\
    \ pile-ons\n{% elif input.focus == 'hate_speech' %}\nLook for: discrimination,\
    \ slurs, dehumanization based on identity\n{% elif input.focus == 'threats' %}\n\
    Look for: threats of violence, harm, doxxing, real-world consequences\n{% elif\
    \ input.focus == 'profanity' %}\nLook for: explicit language, crude terms, vulgar\
    \ expressions\n{% elif input.focus == 'personal_attacks' %}\nLook for: ad hominem\
    \ attacks, insults, character assassination\n{% endif %}\n{% else %}\nAssess OVERALL\
    \ toxicity including: hostility, aggression, incivility, harmful content\n{% endif\
    \ %}\n\nUse {{ input.num_levels | default(5) }} distinct levels:\n\n{% if input.focus\
    \ %}\nLevels for \"{{ input.focus }}\":\n- No {{ input.focus | replace('_', '\
    \ ') | title }}\n- Mild {{ input.focus | replace('_', ' ') | title }}\n- Moderate\
    \ {{ input.focus | replace('_', ' ') | title }}\n- Severe {{ input.focus | replace('_',\
    \ ' ') | title }}\n- Extreme {{ input.focus | replace('_', ' ') | title }}\n{%\
    \ else %}\nToxicity Levels:\n- Civil (constructive, respectful, or neutral)\n\
    - Mildly Uncivil (slightly rude, dismissive)\n- Moderately Toxic (hostile, aggressive,\
    \ insulting)\n- Highly Toxic (very hostile, personal attacks, harmful)\n- Severely\
    \ Toxic (extreme hostility, threats, hate)\n{% endif %}\n\nTexts to analyze (with\
    \ index numbers):\n{% for v in input.texts %}\n[{{ loop.index0 }}] \"{{ v[:500]\
    \ | replace('\"', '\\\\\"') }}\"\n{% endfor %}\n\nReturn a JSON object mapping\
    \ each text's INDEX NUMBER to its toxicity level:\n\n{\n  \"mapping\": {\n   \
    \ \"0\": \"Level Name\",\n    \"1\": \"Level Name\",\n    ...\n  }\n}\n\nRULES:\n\
    - Use the INDEX NUMBER (0, 1, 2, ...) as keys\n- Be consistent with level names\
    \ across all assignments\n- Consider context (criticism ≠ toxicity, debate ≠ harassment)\n\
    - Return ONLY the JSON object\n"
  rules:
    max_turns: 1
    max_attempts: 3
  output_schema:
    type: object
    properties:
      mapping:
        type: object
    required:
    - mapping
